{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8f1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             roc_auc_score, roc_curve, brier_score_loss)\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Boosting libraries\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================= 配置区域 (Configuration) =================\n",
    "# 1. 设置 Times New Roman 字体\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 2. 设置 PDF 导出为矢量格式 (Type 42)\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "# 3. 设置文件路径\n",
    "input_path = \"D:/卒中预后第一篇论文/2025.11.25-new work/3.模型比较/\"\n",
    "output_path = \"D:/卒中预后第一篇论文/2025.11.25-new work/3.模型比较/\"\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "# ===========================================================\n",
    "\n",
    "# 定义分类器列表\n",
    "classifiers = [\n",
    "    ('XGBoost', xgb.XGBClassifier(\n",
    "        learning_rate=0.01, max_delta_step=1, sampling_method='uniform',\n",
    "        random_state=30, eval_metric='logloss', use_label_encoder=False\n",
    "    )),\n",
    "    ('Catboost', cb.CatBoostClassifier(\n",
    "        random_state=30, verbose=0, depth=4, learning_rate=0.01,\n",
    "        l2_leaf_reg=100, subsample=0.7, random_strength=5,\n",
    "        early_stopping_rounds=20, allow_writing_files=False\n",
    "    )),\n",
    "    ('LightGBM', lgb.LGBMClassifier(\n",
    "        random_state=30, num_leaves=32, min_child_samples=100,\n",
    "        learning_rate=0.02, reg_alpha=1, reg_lambda=5, feature_fraction=0.7,\n",
    "        bagging_fraction=0.8, extra_trees=True, n_estimators=200, verbose=-1\n",
    "    )),\n",
    "    ('Logistic Regression', LogisticRegression(random_state=30, max_iter=1000)),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=30)),\n",
    "    ('K Neighbors', KNeighborsClassifier()),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('MLP Classifier', MLPClassifier(random_state=30, max_iter=1000)),\n",
    "    ('Gaussian Process', GaussianProcessClassifier(random_state=30)),\n",
    "    ('QDA', QuadraticDiscriminantAnalysis()),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=30)),\n",
    "    ('Bagging', BaggingClassifier(random_state=30)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=30)),\n",
    "    ('Extra Trees', ExtraTreesClassifier(\n",
    "        random_state=30, n_estimators=300, max_depth=8, min_samples_split=10,\n",
    "        min_samples_leaf=4, max_features=\"sqrt\", bootstrap=True, ccp_alpha=0.03, n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "# ================= 核心计算与绘图函数 =================\n",
    "\n",
    "def setup_plot_for_editable_pdf(figsize=(10, 8)):\n",
    "    \"\"\"初始化绘图，确保图层正确\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.set_axisbelow(True)\n",
    "    return fig, ax\n",
    "\n",
    "# --- 1. DCA (决策曲线) 相关函数 ---\n",
    "def calculate_net_benefit(y_true, y_prob, thresholds):\n",
    "    \"\"\"计算净收益 Net Benefit\"\"\"\n",
    "    net_benefits = []\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    n = len(y_true)\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        # 预测为阳性 (Risky)\n",
    "        y_pred = y_prob >= thresh\n",
    "        \n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        \n",
    "        # Net Benefit 公式: (TP/N) - (FP/N) * (pt / (1-pt))\n",
    "        if thresh == 1.0:\n",
    "            nb = 0 \n",
    "        else:\n",
    "            weight = thresh / (1 - thresh)\n",
    "            nb = (tp / n) - (fp / n) * weight\n",
    "        net_benefits.append(nb)\n",
    "    return np.array(net_benefits)\n",
    "\n",
    "def plot_decision_curves(dca_data, prevalence, filename, cohort_name=\"Internal\"):\n",
    "    \"\"\"绘制决策曲线 (DCA)\"\"\"\n",
    "    fig, ax = setup_plot_for_editable_pdf((10, 8))\n",
    "    \n",
    "    # 阈值范围 1% 到 99%\n",
    "    thresholds = np.linspace(0.01, 0.99, 99)\n",
    "    \n",
    "    # 1. Plot Treat All (All Positive)\n",
    "    # NB_all = Prevalence - (1-Prevalence)* (pt/(1-pt))\n",
    "    treat_all = prevalence - (1 - prevalence) * thresholds / (1 - thresholds)\n",
    "    ax.plot(thresholds, treat_all, linestyle=':', color='gray', label='Treat All', linewidth=1.5)\n",
    "    \n",
    "    # 2. Plot Treat None (All Negative, NB=0)\n",
    "    ax.plot(thresholds, np.zeros_like(thresholds), linestyle='-', color='black', label='Treat None', linewidth=1.5)\n",
    "    \n",
    "    # 3. Plot Models\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(dca_data)))\n",
    "    for (model_name, data), color in zip(dca_data.items(), colors):\n",
    "        y_true_local = data['y_true']\n",
    "        y_prob_local = data['y_prob']\n",
    "        \n",
    "        nb = calculate_net_benefit(y_true_local, y_prob_local, thresholds)\n",
    "        ax.plot(thresholds, nb, color=color, label=model_name, linewidth=2)\n",
    "        \n",
    "    # 动态设置 Y 轴上限 (Prevalence + buffer)\n",
    "    y_max = max(prevalence, 0.1) + 0.15 \n",
    "    ax.set_ylim([-0.05, y_max]) \n",
    "    ax.set_xlim([0, 1.0])\n",
    "    \n",
    "    ax.set_xlabel('Threshold Probability', fontsize=14)\n",
    "    ax.set_ylabel('Net Benefit', fontsize=14)\n",
    "    ax.set_title(f'Decision Curve Analysis - {cohort_name}', fontsize=16, fontweight='bold', pad=15)\n",
    "    \n",
    "    ax.legend(loc='upper right', fontsize=10, frameon=True)\n",
    "    ax.grid(True, linestyle=':', alpha=0.6)\n",
    "    \n",
    "    save_path = os.path.join(output_path, f\"{filename}_{cohort_name.lower()}.pdf\")\n",
    "    plt.savefig(save_path, format='pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# --- 2. ROC 曲线相关函数 ---\n",
    "def plot_combined_roc_curves_with_ci(roc_data, title_suffix, filename):\n",
    "    fig, ax = setup_plot_for_editable_pdf((10, 8))\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.6, label='Chance Level (AUC = 0.500)')\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(roc_data)))\n",
    "    \n",
    "    for (model_name, (fpr, tpr, auc_score, ci_lower, ci_upper)), color in zip(roc_data.items(), colors):\n",
    "        if not np.isnan(auc_score):\n",
    "            label = f'{model_name} (AUC = {auc_score:.3f} [{ci_lower:.3f}-{ci_upper:.3f}])'\n",
    "            ax.plot(fpr, tpr, label=label, linewidth=2, color=color)\n",
    "    \n",
    "    ax.set_xlim([-0.02, 1.02])\n",
    "    ax.set_ylim([-0.02, 1.05])\n",
    "    ax.set_xlabel('1 - Specificity (False Positive Rate)', fontsize=14)\n",
    "    ax.set_ylabel('Sensitivity (True Positive Rate)', fontsize=14)\n",
    "    ax.set_title(f'ROC Curves - {title_suffix}', fontsize=16, fontweight='bold', pad=15)\n",
    "    ax.legend(loc='lower right', fontsize=10, frameon=True)\n",
    "    ax.grid(True, linestyle=':', alpha=0.6)\n",
    "    \n",
    "    save_path = os.path.join(output_path, f\"{filename}.pdf\")\n",
    "    plt.savefig(save_path, format='pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# --- 3. 校准曲线相关函数 ---\n",
    "def plot_enhanced_calibration_curves(calibration_data, title_suffix, filename, cohort_name=\"Internal\"):\n",
    "    fig, ax = setup_plot_for_editable_pdf((10, 8))\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.6, linewidth=1.5, label='Perfectly Calibrated')\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(calibration_data)))\n",
    "    \n",
    "    for (model_name, data), color in zip(calibration_data.items(), colors):\n",
    "        prob_true, prob_pred = data['calibration_curve']\n",
    "        brier = data.get('brier_score', np.nan)\n",
    "        label_str = f'{model_name} (Brier: {brier:.3f})'\n",
    "        ax.plot(prob_pred, prob_true, 's-', color=color, label=label_str, markersize=5, linewidth=1.5)\n",
    "    \n",
    "    ax.set_xlim([-0.02, 1.02])\n",
    "    ax.set_ylim([-0.02, 1.05])\n",
    "    ax.set_xlabel('Predicted Probability', fontsize=14)\n",
    "    ax.set_ylabel('Observed Fraction', fontsize=14)\n",
    "    ax.set_title(f'Calibration Curves - {cohort_name} Cohort', fontsize=16, fontweight='bold', pad=15)\n",
    "    ax.legend(loc='best', fontsize=10, frameon=True)\n",
    "    ax.grid(True, linestyle=':', alpha=0.6)\n",
    "    \n",
    "    save_path = os.path.join(output_path, f\"{filename}_{cohort_name.lower()}.pdf\")\n",
    "    plt.savefig(save_path, format='pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# ================= 统计与指标计算函数 =================\n",
    "def calculate_calibration_metrics(y_true, y_prob, n_bins=10):\n",
    "    brier_score = brier_score_loss(y_true, y_prob)\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='quantile')\n",
    "    try:\n",
    "        epsilon = 1e-10\n",
    "        y_prob_clipped = np.clip(y_prob, epsilon, 1-epsilon)\n",
    "        logit_pred = np.log(y_prob_clipped / (1 - y_prob_clipped)).reshape(-1, 1)\n",
    "        cal_model = LogisticRegression(fit_intercept=True, solver='liblinear')\n",
    "        cal_model.fit(logit_pred, y_true)\n",
    "        calibration_slope = cal_model.coef_[0][0]\n",
    "        calibration_intercept = cal_model.intercept_[0]\n",
    "    except:\n",
    "        calibration_slope, calibration_intercept = np.nan, np.nan\n",
    "    \n",
    "    return {'brier_score': brier_score, 'calibration_slope': calibration_slope,\n",
    "            'calibration_intercept': calibration_intercept, 'calibration_curve': (prob_true, prob_pred)}\n",
    "\n",
    "def calculate_comprehensive_metrics(y_true, y_pred, y_prob=None, n_bootstrap=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    计算所有指标及其 95% 置信区间 (Bootstrap法)\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    if y_prob is not None: y_prob = np.array(y_prob)\n",
    "    \n",
    "    # 辅助函数：计算单次迭代的指标\n",
    "    def compute_single_pass(y_t, y_p, y_pr):\n",
    "        m = {}\n",
    "        m['accuracy'] = accuracy_score(y_t, y_p)\n",
    "        m['precision'] = precision_score(y_t, y_p, average='macro', zero_division=0)\n",
    "        m['recall'] = recall_score(y_t, y_p, average='macro', zero_division=0)\n",
    "        m['f1_macro'] = f1_score(y_t, y_p, average='macro', zero_division=0)\n",
    "        if y_pr is not None and len(np.unique(y_t)) == 2:\n",
    "            try:\n",
    "                m['auc'] = roc_auc_score(y_t, y_pr)\n",
    "                m['brier_score'] = brier_score_loss(y_t, y_pr)\n",
    "            except: m['auc'], m['brier_score'] = np.nan, np.nan\n",
    "        else: m['auc'], m['brier_score'] = np.nan, np.nan\n",
    "        return m\n",
    "\n",
    "    # 1. 计算点估计值\n",
    "    point_metrics = compute_single_pass(y_true, y_pred, y_prob)\n",
    "    # 校准斜率/截距通常不进行 Bootstrap\n",
    "    if y_prob is not None and len(np.unique(y_true)) == 2:\n",
    "        point_metrics.update(calculate_calibration_metrics(y_true, y_prob))\n",
    "    \n",
    "    # 2. Bootstrap 计算置信区间\n",
    "    boot_scores = defaultdict(list)\n",
    "    for i in range(n_bootstrap):\n",
    "        indices = resample(range(len(y_true)), replace=True, random_state=i)\n",
    "        # 确保 Bootstrap 样本中包含两个类别\n",
    "        if len(np.unique(y_true[indices])) < 2: continue\n",
    "        \n",
    "        try:\n",
    "            m = compute_single_pass(y_true[indices], y_pred[indices], y_prob[indices] if y_prob is not None else None)\n",
    "            for k, v in m.items():\n",
    "                if not np.isnan(v): boot_scores[k].append(v)\n",
    "        except: continue\n",
    "            \n",
    "    final_metrics = point_metrics.copy()\n",
    "    # 计算 95% CI\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1_macro', 'auc', 'brier_score']:\n",
    "        scores = boot_scores.get(metric, [])\n",
    "        if scores:\n",
    "            final_metrics[f'{metric}_ci_lower'] = np.percentile(scores, 100 * alpha / 2)\n",
    "            final_metrics[f'{metric}_ci_upper'] = np.percentile(scores, 100 * (1 - alpha / 2))\n",
    "        else:\n",
    "            final_metrics[f'{metric}_ci_lower'], final_metrics[f'{metric}_ci_upper'] = np.nan, np.nan\n",
    "            \n",
    "    return final_metrics\n",
    "\n",
    "def delong_test(y_true, y_prob1, y_prob2):\n",
    "    y_true = np.array(y_true).reshape(-1)\n",
    "    y_prob1 = np.array(y_prob1).reshape(-1)\n",
    "    y_prob2 = np.array(y_prob2).reshape(-1)\n",
    "    auc1 = roc_auc_score(y_true, y_prob1)\n",
    "    auc2 = roc_auc_score(y_true, y_prob2)\n",
    "\n",
    "    def compute_covariance(y_true, p1, p2):\n",
    "        idx_pos = y_true == 1\n",
    "        idx_neg = y_true == 0\n",
    "        V10_1, V01_1 = np.zeros(idx_pos.sum()), np.zeros(idx_neg.sum())\n",
    "        V10_2, V01_2 = np.zeros(idx_pos.sum()), np.zeros(idx_neg.sum())\n",
    "        p1_pos, p1_neg = p1[idx_pos], p1[idx_neg]\n",
    "        p2_pos, p2_neg = p2[idx_pos], p2[idx_neg]\n",
    "        \n",
    "        for i in range(len(p1_pos)): V10_1[i] = np.mean(p1_pos[i] > p1_neg) + 0.5 * np.mean(p1_pos[i] == p1_neg)\n",
    "        for j in range(len(p1_neg)): V01_1[j] = np.mean(p1_pos > p1_neg[j]) + 0.5 * np.mean(p1_pos == p1_neg[j])\n",
    "        for i in range(len(p2_pos)): V10_2[i] = np.mean(p2_pos[i] > p2_neg) + 0.5 * np.mean(p2_pos[i] == p2_neg)\n",
    "        for j in range(len(p2_neg)): V01_2[j] = np.mean(p2_pos > p2_neg[j]) + 0.5 * np.mean(p2_pos == p2_neg[j])\n",
    "\n",
    "        var1 = (np.var(V10_1, ddof=1)/idx_pos.sum() + np.var(V01_1, ddof=1)/idx_neg.sum())\n",
    "        var2 = (np.var(V10_2, ddof=1)/idx_pos.sum() + np.var(V01_2, ddof=1)/idx_neg.sum())\n",
    "        cov_12 = (np.cov(V10_1, V10_2)[0, 1]/idx_pos.sum() + np.cov(V01_1, V01_2)[0, 1]/idx_neg.sum())\n",
    "        return var1, var2, cov_12\n",
    "\n",
    "    var1, var2, cov_12 = compute_covariance(y_true, y_prob1, y_prob2)\n",
    "    z = (auc1 - auc2) / np.sqrt(var1 + var2 - 2 * cov_12 + 1e-8)\n",
    "    return z, 2 * (1 - norm.cdf(abs(z)))\n",
    "\n",
    "def load_external_cohorts():\n",
    "    external_data = {}\n",
    "    external_files = {\n",
    "        'External_Cohort': os.path.join(input_path, \"外部验证集-量子计算.xlsx\"),\n",
    "    }\n",
    "    for cohort_name, file_path in external_files.items():\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                data = pd.read_excel(file_path)\n",
    "                external_data[cohort_name] = data\n",
    "                print(f\"Loaded external cohort: {cohort_name}\")\n",
    "            except Exception as e: print(f\"Failed to load {cohort_name}: {e}\")\n",
    "        else: print(f\"External file not found: {file_path}\")\n",
    "    return external_data\n",
    "\n",
    "def save_comprehensive_results_to_excel(internal_results, external_results, delong_results, filename):\n",
    "    filepath = os.path.join(output_path, filename)\n",
    "    \n",
    "    def format_metrics(metrics_dict):\n",
    "        \"\"\"Helper to format row data with CIs\"\"\"\n",
    "        row = {}\n",
    "        for k in ['auc', 'accuracy', 'precision', 'recall', 'f1_macro', 'brier_score']:\n",
    "            val = metrics_dict.get(k)\n",
    "            low = metrics_dict.get(f'{k}_ci_lower')\n",
    "            high = metrics_dict.get(f'{k}_ci_upper')\n",
    "            \n",
    "            row[f'{k.upper()}'] = val\n",
    "            row[f'{k.upper()}_95CI_Low'] = low\n",
    "            row[f'{k.upper()}_95CI_High'] = high\n",
    "            \n",
    "            # 创建一个组合字符串列，方便直接复制到论文\n",
    "            if val is not None and low is not None:\n",
    "                row[f'{k.upper()}_Str'] = f\"{val:.3f} ({low:.3f}-{high:.3f})\"\n",
    "                \n",
    "        row['Slope'] = metrics_dict.get('calibration_slope')\n",
    "        row['Intercept'] = metrics_dict.get('calibration_intercept')\n",
    "        return row\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "            # 保存内部验证结果\n",
    "            int_summary = []\n",
    "            for m_name, res in internal_results.items():\n",
    "                row = {'Model': m_name, 'Cohort': 'Internal'}\n",
    "                row.update(format_metrics(res['test_metrics']))\n",
    "                int_summary.append(row)\n",
    "            if int_summary: \n",
    "                pd.DataFrame(int_summary).to_excel(writer, sheet_name='Internal_Results', index=False)\n",
    "            \n",
    "            # 保存外部验证结果\n",
    "            ext_summary = []\n",
    "            for c_name, c_res in external_results.items():\n",
    "                for m_name, res in c_res.items():\n",
    "                    row = {'Model': m_name, 'Cohort': c_name}\n",
    "                    row.update(format_metrics(res['metrics']))\n",
    "                    ext_summary.append(row)\n",
    "            if ext_summary: \n",
    "                pd.DataFrame(ext_summary).to_excel(writer, sheet_name='External_Results', index=False)\n",
    "            \n",
    "            # 保存 DeLong 测试结果\n",
    "            if delong_results:\n",
    "                d_data = [{'Comparison': k, 'Z': v['z_statistic'], 'P_Value': v['p_value'], 'Sig': v['significant']} \n",
    "                          for k, v in delong_results.items()]\n",
    "                pd.DataFrame(d_data).to_excel(writer, sheet_name='DeLong_Tests', index=False)\n",
    "                \n",
    "        print(f\"Results saved to: {filepath}\")\n",
    "    except Exception as e: print(f\"Error saving Excel: {e}\")\n",
    "\n",
    "# ---------------------------- 主程序 (Main) ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Full Analysis (Internal & External Validation with ROC/DCA/CI)...\")\n",
    "    \n",
    "    # 1. 加载内部数据\n",
    "    internal_file = os.path.join(input_path, \"训练集-量子计算.xlsx\")\n",
    "    if not os.path.exists(internal_file):\n",
    "        print(f\"Error: File not found {internal_file}\")\n",
    "        exit(1)\n",
    "    \n",
    "    data = pd.read_excel(internal_file)\n",
    "    if 'Prognosis' not in data.columns:\n",
    "        print(\"Error: 'Prognosis' column not found in internal data.\")\n",
    "        exit(1)\n",
    "\n",
    "    X = data.drop(columns=['Prognosis'])\n",
    "    y = data['Prognosis']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=30, stratify=y)\n",
    "    \n",
    "    # 结果容器\n",
    "    comprehensive_results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    # 内部绘图数据容器\n",
    "    internal_roc_data = {}\n",
    "    internal_cal_data = {}\n",
    "    internal_dca_data = {} \n",
    "    prob_dict = {}\n",
    "    \n",
    "    # 2. 训练与内部验证\n",
    "    print(\"\\n--- Training Models & Internal Validation ---\")\n",
    "    for name, model in classifiers:\n",
    "        try:\n",
    "            print(f\"Training: {name}\")\n",
    "            model.fit(X_train, y_train)\n",
    "            trained_models[name] = model\n",
    "            \n",
    "            # 预测\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_prob = None\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_prob = model.predict_proba(X_test)[:, 1]\n",
    "                prob_dict[name] = y_prob\n",
    "            \n",
    "            # 计算全指标 (含CI)\n",
    "            metrics = calculate_comprehensive_metrics(y_test, y_pred, y_prob, n_bootstrap=1000)\n",
    "            comprehensive_results[name] = {'test_metrics': metrics, 'model': model}\n",
    "            \n",
    "            if y_prob is not None:\n",
    "                # 收集内部 ROC 数据\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "                internal_roc_data[name] = (fpr, tpr, metrics.get('auc'), \n",
    "                                           metrics.get('auc_ci_lower'), metrics.get('auc_ci_upper'))\n",
    "                \n",
    "                # 收集内部校准数据\n",
    "                if 'calibration_curve' in metrics:\n",
    "                    internal_cal_data[name] = metrics\n",
    "                else:\n",
    "                    internal_cal_data[name] = calculate_calibration_metrics(y_test, y_prob)\n",
    "                \n",
    "                # 收集内部 DCA 数据\n",
    "                internal_dca_data[name] = {'y_true': y_test, 'y_prob': y_prob}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed {name}: {e}\")\n",
    "\n",
    "    # 3. 绘制内部图表 (ROC, Calibration, DCA)\n",
    "    if internal_roc_data:\n",
    "        plot_combined_roc_curves_with_ci(internal_roc_data, \"Internal Cohort\", \"ROC_Internal\")\n",
    "    if internal_cal_data:\n",
    "        plot_enhanced_calibration_curves(internal_cal_data, \"\", \"Calibration_Internal\", \"Internal\")\n",
    "    if internal_dca_data:\n",
    "        # 计算内部患病率 (用于 DCA 的 Treat All 线)\n",
    "        prevalence_internal = np.mean(y_test)\n",
    "        plot_decision_curves(internal_dca_data, prevalence_internal, \"DCA_Curve\", \"Internal\")\n",
    "\n",
    "    # 4. 外部验证 (External Validation)\n",
    "    print(\"\\n--- External Validation ---\")\n",
    "    external_data_dict = load_external_cohorts()\n",
    "    external_results = {}\n",
    "    \n",
    "    if external_data_dict:\n",
    "        for cohort_name, df_ext in external_data_dict.items():\n",
    "            if 'Prognosis' not in df_ext.columns: \n",
    "                print(f\"Skipping {cohort_name}: Missing 'Prognosis' column\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing {cohort_name}...\")\n",
    "            X_ext = df_ext.drop(columns=['Prognosis'])\n",
    "            y_ext = df_ext['Prognosis']\n",
    "            \n",
    "            cohort_results = {}\n",
    "            \n",
    "            # 外部绘图数据容器\n",
    "            ext_roc_data = {}\n",
    "            ext_cal_data = {}\n",
    "            ext_dca_data = {}\n",
    "            \n",
    "            for name, model in trained_models.items():\n",
    "                try:\n",
    "                    y_pred_ext = model.predict(X_ext)\n",
    "                    y_prob_ext = None\n",
    "                    if hasattr(model, \"predict_proba\"):\n",
    "                        y_prob_ext = model.predict_proba(X_ext)[:, 1]\n",
    "                    \n",
    "                    # 计算外部全指标 (含CI)\n",
    "                    metrics = calculate_comprehensive_metrics(y_ext, y_pred_ext, y_prob_ext, n_bootstrap=1000)\n",
    "                    cohort_results[name] = {'metrics': metrics}\n",
    "                    \n",
    "                    if y_prob_ext is not None:\n",
    "                        # 收集外部 ROC 数据\n",
    "                        fpr, tpr, _ = roc_curve(y_ext, y_prob_ext)\n",
    "                        ext_roc_data[name] = (fpr, tpr, metrics.get('auc'),\n",
    "                                              metrics.get('auc_ci_lower'), metrics.get('auc_ci_upper'))\n",
    "                        \n",
    "                        # 收集外部校准数据\n",
    "                        if 'calibration_curve' in metrics:\n",
    "                            ext_cal_data[name] = metrics\n",
    "                        else:\n",
    "                            ext_cal_data[name] = calculate_calibration_metrics(y_ext, y_prob_ext)\n",
    "                            \n",
    "                        # 收集外部 DCA 数据\n",
    "                        ext_dca_data[name] = {'y_true': y_ext, 'y_prob': y_prob_ext}\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating {name} on {cohort_name}: {e}\")\n",
    "            \n",
    "            external_results[cohort_name] = cohort_results\n",
    "            \n",
    "            # 绘制外部图表 (ROC, Calibration, DCA)\n",
    "            if ext_roc_data:\n",
    "                plot_combined_roc_curves_with_ci(ext_roc_data, f\"{cohort_name}\", f\"ROC_{cohort_name}\")\n",
    "            if ext_cal_data:\n",
    "                plot_enhanced_calibration_curves(ext_cal_data, \"\", f\"Calibration_{cohort_name}\", cohort_name)\n",
    "            if ext_dca_data:\n",
    "                prevalence_ext = np.mean(y_ext)\n",
    "                plot_decision_curves(ext_dca_data, prevalence_ext, \"DCA_Curve\", cohort_name)\n",
    "\n",
    "    # 5. DeLong 检验与 Excel 保存\n",
    "    delong_results = perform_delong_tests(y_test, prob_dict)\n",
    "    save_comprehensive_results_to_excel(comprehensive_results, external_results, \n",
    "                                      delong_results, \"Model_Evaluation_Full.xlsx\")\n",
    "    \n",
    "    print(\"\\nAnalysis Complete.\")\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "    print(\"Generated files include: Excel metrics, Internal/External ROC plots, DCA plots, and Calibration plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeeda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================\n",
    "# 将此代码块添加到原代码的最后，用于保存预测概率，供后续 DeLong 脚本独立使用\n",
    "# ==========================================================================\n",
    "\n",
    "def save_predictions_for_delong(y_true, prob_dict, filename):\n",
    "    \"\"\"将真实标签和各模型的预测概率保存为Excel\"\"\"\n",
    "    df = pd.DataFrame({'True_Label': y_true})\n",
    "    # 将字典中的概率合并进来\n",
    "    for model_name, probs in prob_dict.items():\n",
    "        if probs is not None:\n",
    "            df[model_name] = probs\n",
    "    \n",
    "    save_file = os.path.join(output_path, filename)\n",
    "    df.to_excel(save_file, index=False)\n",
    "    print(f\"Predictions saved for DeLong test: {save_file}\")\n",
    "\n",
    "# 1. 保存内部验证集预测结果\n",
    "if prob_dict:\n",
    "    save_predictions_for_delong(y_test, prob_dict, \"Predictions_Internal.xlsx\")\n",
    "\n",
    "# 2. 保存外部验证集预测结果\n",
    "if external_results:\n",
    "    for cohort_name, data in load_external_cohorts().items():\n",
    "        if 'Prognosis' not in data.columns: continue\n",
    "        \n",
    "        X_ext = data.drop(columns=['Prognosis'])\n",
    "        y_ext = data['Prognosis']\n",
    "        \n",
    "        ext_prob_dict = {}\n",
    "        for name, model in trained_models.items():\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                ext_prob_dict[name] = model.predict_proba(X_ext)[:, 1]\n",
    "        \n",
    "        save_predictions_for_delong(y_ext, ext_prob_dict, f\"Predictions_{cohort_name}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9099d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ================= 配置区域 (Configuration) =================\n",
    "# 请确保此路径是正确的，并且文件是通过第一步代码生成的\n",
    "target_file_path = \"D:/卒中预后第一篇论文/2025.11.25-new work/3.模型比较/Predictions_External_Cohort.xlsx\"\n",
    "output_path = os.path.dirname(target_file_path)\n",
    "\n",
    "# 绘图设置\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "# ===========================================================\n",
    "\n",
    "def delong_test_stat(y_true, y_prob1, y_prob2):\n",
    "    \"\"\"计算 DeLong Z 统计量和 P 值\"\"\"\n",
    "    y_true = np.array(y_true).reshape(-1)\n",
    "    y_prob1 = np.array(y_prob1).reshape(-1)\n",
    "    y_prob2 = np.array(y_prob2).reshape(-1)\n",
    "\n",
    "    auc1 = roc_auc_score(y_true, y_prob1)\n",
    "    auc2 = roc_auc_score(y_true, y_prob2)\n",
    "\n",
    "    def compute_covariance(y_true, p1, p2):\n",
    "        idx_pos = y_true == 1\n",
    "        idx_neg = y_true == 0\n",
    "        p1_pos, p1_neg = p1[idx_pos], p1[idx_neg]\n",
    "        p2_pos, p2_neg = p2[idx_pos], p2[idx_neg]\n",
    "        n_pos, n_neg = len(p1_pos), len(p1_neg)\n",
    "\n",
    "        def get_structural_components(p_pos, p_neg):\n",
    "            V10 = np.zeros(n_pos)\n",
    "            V01 = np.zeros(n_neg)\n",
    "            for i in range(n_pos):\n",
    "                V10[i] = np.mean(p_pos[i] > p_neg) + 0.5 * np.mean(p_pos[i] == p_neg)\n",
    "            for j in range(n_neg):\n",
    "                V01[j] = np.mean(p_pos > p_neg[j]) + 0.5 * np.mean(p_pos == p_neg[j])\n",
    "            return V10, V01\n",
    "\n",
    "        V10_1, V01_1 = get_structural_components(p1_pos, p1_neg)\n",
    "        V10_2, V01_2 = get_structural_components(p2_pos, p2_neg)\n",
    "\n",
    "        var1 = (np.var(V10_1, ddof=1) / n_pos + np.var(V01_1, ddof=1) / n_neg)\n",
    "        var2 = (np.var(V10_2, ddof=1) / n_pos + np.var(V01_2, ddof=1) / n_neg)\n",
    "        cov_12 = (np.cov(V10_1, V10_2)[0, 1] / n_pos + np.cov(V01_1, V01_2)[0, 1] / n_neg)\n",
    "        return var1, var2, cov_12\n",
    "\n",
    "    var1, var2, cov_12 = compute_covariance(y_true, y_prob1, y_prob2)\n",
    "    \n",
    "    sigma = np.sqrt(var1 + var2 - 2 * cov_12 + 1e-8)\n",
    "    z = (auc1 - auc2) / sigma\n",
    "    p_value = 2 * (1 - norm.cdf(abs(z)))\n",
    "    \n",
    "    return z, p_value, auc1, auc2\n",
    "\n",
    "def plot_directional_heatmap(df_probs, title, filename):\n",
    "    \"\"\"绘制带有方向性的热图\"\"\"\n",
    "    y_true = df_probs['True_Label'].values\n",
    "    models = [c for c in df_probs.columns if c != 'True_Label']\n",
    "    \n",
    "    # 按 AUC 从高到低排序模型\n",
    "    auc_scores = {m: roc_auc_score(y_true, df_probs[m]) for m in models}\n",
    "    models = sorted(models, key=lambda x: auc_scores[x], reverse=True)\n",
    "    \n",
    "    n_models = len(models)\n",
    "    z_matrix = pd.DataFrame(np.nan, index=models, columns=models)\n",
    "    text_matrix = pd.DataFrame(\"\", index=models, columns=models)\n",
    "    \n",
    "    print(f\"Computing pairwise tests for {n_models} models...\")\n",
    "    \n",
    "    for i, m1 in enumerate(models):\n",
    "        for j, m2 in enumerate(models):\n",
    "            if i == j: continue\n",
    "            \n",
    "            z, p, _, _ = delong_test_stat(y_true, df_probs[m1], df_probs[m2])\n",
    "            z_matrix.loc[m1, m2] = z\n",
    "            \n",
    "            # 设置文本：P值，显著则加星号\n",
    "            p_str = \"<0.001\" if p < 0.001 else f\"{p:.3f}\"\n",
    "            if p < 0.05: p_str += \"*\"\n",
    "            text_matrix.loc[m1, m2] = p_str\n",
    "\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # 设置颜色范围\n",
    "    max_z = np.nanmax(np.abs(z_matrix.values))\n",
    "    limit = max(3, max_z)\n",
    "    \n",
    "    # 修复了之前的报错：确保 cbar_kws 中的字符串在同一行，且不过长\n",
    "    heatmap_label = \"Z-Score (Red: Row > Col, Blue: Row < Col)\"\n",
    "    \n",
    "    ax = sns.heatmap(z_matrix, \n",
    "                     annot=text_matrix, \n",
    "                     fmt=\"\", \n",
    "                     cmap='RdBu_r', \n",
    "                     center=0,\n",
    "                     vmin=-limit, vmax=limit,\n",
    "                     square=True, \n",
    "                     linewidths=.5, \n",
    "                     cbar_kws={\"label\": heatmap_label})\n",
    "    \n",
    "    plt.title(f'Pairwise DeLong Test Comparison\\n({title})', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel(\"Comparison Model (Column)\", fontsize=12)\n",
    "    plt.ylabel(\"Reference Model (Row)\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(rotation=0, fontsize=10)\n",
    "    \n",
    "    # 添加底部注解\n",
    "    plt.figtext(0.5, 0.01, \"* P < 0.05 | Red: Row Better | Blue: Row Worse\", \n",
    "                ha=\"center\", fontsize=10, bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5})\n",
    "    \n",
    "    save_file = os.path.join(output_path, filename)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_file, format='pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Heatmap saved to: {save_file}\")\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(target_file_path):\n",
    "        print(f\"Error: File not found {target_file_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading predictions from: {target_file_path}\")\n",
    "    df = pd.read_excel(target_file_path)\n",
    "    base_name = os.path.basename(target_file_path).replace('.xlsx', '')\n",
    "    plot_directional_heatmap(df, base_name, f\"{base_name}_DeLong_Heatmap.pdf\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Sklearn & Models\n",
    "from sklearn.metrics import (roc_curve, auc, roc_auc_score, f1_score, accuracy_score, \n",
    "                             precision_score, recall_score, confusion_matrix, brier_score_loss)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Boosting Libraries\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "# ======================== 1. 基础计算与指标函数 ========================\n",
    "\n",
    "def bootstrap_ci(y_true, y_prob, metric_func, n_bootstraps=1000, alpha=0.05, **kwargs):\n",
    "    \"\"\"Bootstrap计算95% CI\"\"\"\n",
    "    rng = np.random.RandomState(42)\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(n_bootstraps):\n",
    "        indices = rng.randint(0, len(y_prob), len(y_prob))\n",
    "        if len(np.unique(y_true[indices])) < 2: continue\n",
    "        try:\n",
    "            scores.append(metric_func(y_true[indices], y_prob[indices], **kwargs))\n",
    "        except: continue\n",
    "            \n",
    "    if not scores: return np.nan, np.nan\n",
    "    scores = np.sort(scores)\n",
    "    return np.percentile(scores, alpha/2*100), np.percentile(scores, (1-alpha/2)*100)\n",
    "\n",
    "def calculate_metrics(y_true, y_prob, threshold):\n",
    "    \"\"\"计算指定阈值下的单次指标 (不含CI，用于循环中快速计算权重)\"\"\"\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sen = recall_score(y_true, y_pred, zero_division=0)\n",
    "    spe = tn / (tn + fp + 1e-10)\n",
    "    ppv = precision_score(y_true, y_pred, zero_division=0)\n",
    "    npv = tn / (tn + fn + 1e-10)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # AUC 和 Brier 与阈值无关，但在表中展示作为参考\n",
    "    auc_val = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    \n",
    "    return {\n",
    "        'AUC': auc_val, 'F1': f1, 'ACC': acc, \n",
    "        'SEN': sen, 'SPE': spe, 'PPV': ppv, 'NPV': npv\n",
    "    }\n",
    "\n",
    "def get_full_metrics_with_ci(y_true, y_prob, threshold):\n",
    "    \"\"\"计算完整指标含CI (用于最终报告)\"\"\"\n",
    "    base = calculate_metrics(y_true, y_prob, threshold)\n",
    "    \n",
    "    # 补充 Brier\n",
    "    base['Brier'] = brier_score_loss(y_true, y_prob)\n",
    "    \n",
    "    # 计算 AUC CI\n",
    "    low, high = bootstrap_ci(y_true, y_prob, roc_auc_score, n_bootstraps=500)\n",
    "    base['AUC_95CI'] = f\"{base['AUC']:.3f} ({low:.3f}-{high:.3f})\"\n",
    "    \n",
    "    # 计算 Brier CI\n",
    "    b_low, b_high = bootstrap_ci(y_true, y_prob, brier_score_loss, n_bootstraps=500)\n",
    "    base['Brier_95CI'] = f\"{base['Brier']:.3f} ({b_low:.3f}-{b_high:.3f})\"\n",
    "    \n",
    "    return base\n",
    "\n",
    "# ======================== 2. CRITIC 权重计算 ========================\n",
    "\n",
    "def critic_weighting(metrics_df):\n",
    "    \"\"\"\n",
    "    基于当前指标数据框计算 CRITIC 权重\n",
    "    metrics_df: 行是模型，列是指标 (AUC, F1, ACC, SEN, SPE)\n",
    "    \"\"\"\n",
    "    df = metrics_df.copy()\n",
    "    # 选取用于权重的指标 (正向指标)\n",
    "    indicators = ['AUC', 'F1', 'ACC', 'SEN', 'SPE']\n",
    "    valid_inds = [c for c in indicators if c in df.columns]\n",
    "    \n",
    "    # 获取数据矩阵\n",
    "    Z = df[valid_inds].values\n",
    "    \n",
    "    # 极差归一化\n",
    "    ptp = Z.max(axis=0) - Z.min(axis=0)\n",
    "    ptp[ptp == 0] = 1e-8 # 防止除零\n",
    "    Z_norm = (Z - Z.min(axis=0)) / ptp\n",
    "    \n",
    "    # 标准差 (对比强度)\n",
    "    std_dev = np.std(Z_norm, axis=0)\n",
    "    \n",
    "    # 相关系数 (冲突性)\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        corr_matrix = np.corrcoef(Z_norm, rowvar=False)\n",
    "    if np.isnan(corr_matrix).any(): \n",
    "        # 如果某列全是0(如阈值极端时)，相关系数无法计算，设冲突系数为1(无冲突信息)\n",
    "        conflict = np.ones(len(valid_inds))\n",
    "    else:\n",
    "        conflict = np.sum(1 - np.abs(corr_matrix), axis=1)\n",
    "    \n",
    "    # 信息量\n",
    "    info = std_dev * conflict\n",
    "    \n",
    "    # 指标权重\n",
    "    w_j = info / (np.sum(info) + 1e-10)\n",
    "    \n",
    "    # 模型综合得分\n",
    "    model_scores = np.sum(Z_norm * w_j, axis=1)\n",
    "    \n",
    "    # 归一化为模型权重\n",
    "    final_model_weights = model_scores / (np.sum(model_scores) + 1e-10)\n",
    "    \n",
    "    return dict(zip(df['Classifier'], final_model_weights))\n",
    "\n",
    "def get_ensemble_probs(probs_dict, weights):\n",
    "    \"\"\"根据权重组合概率\"\"\"\n",
    "    ensemble_probs = np.zeros_like(list(probs_dict.values())[0])\n",
    "    total_weight = 0\n",
    "    for name, w in weights.items():\n",
    "        if name in probs_dict:\n",
    "            ensemble_probs += w * probs_dict[name]\n",
    "            total_weight += w\n",
    "    return ensemble_probs / (total_weight + 1e-10)\n",
    "\n",
    "# ======================== 3. 绘图函数 ========================\n",
    "\n",
    "def plot_roc_curves(y_true, probs_dict, save_path, title_suffix=\"\"):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(probs_dict)))\n",
    "    \n",
    "    for i, (name, probs) in enumerate(probs_dict.items()):\n",
    "        fpr, tpr, _ = roc_curve(y_true, probs)\n",
    "        auc_val = roc_auc_score(y_true, probs)\n",
    "        plt.plot(fpr, tpr, color=colors[i], lw=2, label=f'{name} (AUC={auc_val:.3f})')\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves - {title_suffix}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(save_path.replace('.png', '.pdf'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_calibration(y_true, probs_dict, save_path, title_suffix=\"\"):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(probs_dict)))\n",
    "    \n",
    "    for i, (name, probs) in enumerate(probs_dict.items()):\n",
    "        frac, mean_val = calibration_curve(y_true, probs, n_bins=10)\n",
    "        brier = brier_score_loss(y_true, probs)\n",
    "        plt.plot(mean_val, frac, \"s-\", color=colors[i], label=f\"{name} (Brier: {brier:.3f})\")\n",
    "    \n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title(f'Calibration - {title_suffix}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(save_path.replace('.png', '.pdf'))\n",
    "    plt.close()\n",
    "\n",
    "# ======================== 4. 主逻辑流程 ========================\n",
    "\n",
    "def train_base_models(X_train, y_train, classifiers):\n",
    "    models = {}\n",
    "    for name, clf in classifiers:\n",
    "        print(f\"  Training {name}...\")\n",
    "        model = clone(clf)\n",
    "        model.fit(X_train, y_train)\n",
    "        models[name] = model\n",
    "    return models\n",
    "\n",
    "def run_dynamic_threshold_analysis(models, X_int, y_int, X_ext, y_ext, save_dir):\n",
    "    \"\"\"\n",
    "    核心函数：遍历阈值 -> 动态计算权重 -> 动态构建COPE -> 评估内外部\n",
    "    \"\"\"\n",
    "    # 1. 预先计算所有基础模型的概率 (避免循环中重复预测)\n",
    "    base_probs_int = {}\n",
    "    base_probs_ext = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            base_probs_int[name] = model.predict_proba(X_int)[:, 1]\n",
    "            base_probs_ext[name] = model.predict_proba(X_ext)[:, 1]\n",
    "        else:\n",
    "            base_probs_int[name] = model.predict(X_int)\n",
    "            base_probs_ext[name] = model.predict(X_ext)\n",
    "            \n",
    "    # 2. 遍历阈值\n",
    "    thresholds = np.arange(0.05, 0.96, 0.05)\n",
    "    dynamic_results = []\n",
    "    best_f1 = -1\n",
    "    best_threshold = 0.5\n",
    "    best_weights = None\n",
    "    \n",
    "    print(\"  Running dynamic threshold loop...\")\n",
    "    for thresh in thresholds:\n",
    "        thresh = round(thresh, 2)\n",
    "        \n",
    "        # A. 计算内部集上基础模型的指标 (用于权重)\n",
    "        model_metrics_list = []\n",
    "        for name in models.keys():\n",
    "            m = calculate_metrics(y_int, base_probs_int[name], thresh)\n",
    "            m['Classifier'] = name\n",
    "            model_metrics_list.append(m)\n",
    "        \n",
    "        metrics_df = pd.DataFrame(model_metrics_list)\n",
    "        \n",
    "        # B. 动态计算 CRITIC 权重\n",
    "        current_weights = critic_weighting(metrics_df)\n",
    "        \n",
    "        # C. 构建动态 COPE\n",
    "        cope_prob_int = get_ensemble_probs(base_probs_int, current_weights)\n",
    "        cope_prob_ext = get_ensemble_probs(base_probs_ext, current_weights)\n",
    "        \n",
    "        # D. 评估 COPE (内部 & 外部)\n",
    "        cope_res_int = calculate_metrics(y_int, cope_prob_int, thresh)\n",
    "        cope_res_ext = calculate_metrics(y_ext, cope_prob_ext, thresh)\n",
    "        \n",
    "        # 记录结果\n",
    "        row = {'Threshold': thresh}\n",
    "        # 记录权重\n",
    "        for k, v in current_weights.items():\n",
    "            row[f'Weight_{k}'] = v\n",
    "        # 记录内部指标\n",
    "        for k, v in cope_res_int.items():\n",
    "            row[f'Int_{k}'] = v\n",
    "        # 记录外部指标\n",
    "        for k, v in cope_res_ext.items():\n",
    "            row[f'Ext_{k}'] = v\n",
    "            \n",
    "        dynamic_results.append(row)\n",
    "        \n",
    "        # 记录最佳阈值 (以内部 F1 为准)\n",
    "        if cope_res_int['F1'] > best_f1:\n",
    "            best_f1 = cope_res_int['F1']\n",
    "            best_threshold = thresh\n",
    "            best_weights = current_weights\n",
    "            \n",
    "    # 3. 保存动态分析大表\n",
    "    analysis_df = pd.DataFrame(dynamic_results)\n",
    "    analysis_df.to_excel(os.path.join(save_dir, 'Dynamic_Threshold_Analysis.xlsx'), index=False)\n",
    "    \n",
    "    # 4. 基于最佳阈值生成最终详细报告和图表\n",
    "    print(f\"  Best Threshold found: {best_threshold} (Internal F1: {best_f1:.3f})\")\n",
    "    \n",
    "    # 构建最佳 COPE 概率\n",
    "    final_cope_prob_int = get_ensemble_probs(base_probs_int, best_weights)\n",
    "    final_cope_prob_ext = get_ensemble_probs(base_probs_ext, best_weights)\n",
    "    \n",
    "    # 准备绘图数据 (加入 COPE)\n",
    "    plot_probs_int = base_probs_int.copy()\n",
    "    plot_probs_int['COPE'] = final_cope_prob_int\n",
    "    \n",
    "    plot_probs_ext = base_probs_ext.copy()\n",
    "    plot_probs_ext['COPE'] = final_cope_prob_ext\n",
    "    \n",
    "    # 生成图表\n",
    "    plot_roc_curves(y_int, plot_probs_int, os.path.join(save_dir, 'Internal_ROC.png'), \"Internal\")\n",
    "    plot_roc_curves(y_ext, plot_probs_ext, os.path.join(save_dir, 'External_ROC.png'), \"External\")\n",
    "    \n",
    "    plot_calibration(y_int, plot_probs_int, os.path.join(save_dir, 'Internal_Calibration.png'), \"Internal\")\n",
    "    plot_calibration(y_ext, plot_probs_ext, os.path.join(save_dir, 'External_Calibration.png'), \"External\")\n",
    "    \n",
    "    # 生成详细指标表 (含CI)\n",
    "    final_metrics = []\n",
    "    # 内部\n",
    "    for name, p in plot_probs_int.items():\n",
    "        m = get_full_metrics_with_ci(y_int, p, best_threshold)\n",
    "        m['Classifier'] = name\n",
    "        m['Dataset'] = 'Internal'\n",
    "        final_metrics.append(m)\n",
    "    # 外部\n",
    "    for name, p in plot_probs_ext.items():\n",
    "        m = get_full_metrics_with_ci(y_ext, p, best_threshold)\n",
    "        m['Classifier'] = name\n",
    "        m['Dataset'] = 'External'\n",
    "        final_metrics.append(m)\n",
    "        \n",
    "    pd.DataFrame(final_metrics).to_excel(os.path.join(save_dir, 'Final_Optimal_Metrics.xlsx'), index=False)\n",
    "    \n",
    "    # 绘制权重变化图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    weight_cols = [c for c in analysis_df.columns if c.startswith('Weight_')]\n",
    "    for col in weight_cols:\n",
    "        plt.plot(analysis_df['Threshold'], analysis_df[col], label=col.replace('Weight_', ''))\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('CRITIC Weight')\n",
    "    plt.title('Dynamic CRITIC Weights vs Threshold')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'Weights_Dynamics.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# ======================== 5. 执行入口 ========================\n",
    "\n",
    "classifiers = [\n",
    "    ('Extra Trees', ExtraTreesClassifier(n_estimators=100, random_state=42)),\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ('LightGBM', lgb.LGBMClassifier(random_state=42, verbose=-1)),\n",
    "    ('CatBoost', cb.CatBoostClassifier(random_state=42, verbose=0, allow_writing_files=False)),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    work_dir = r\"D:\\卒中预后第一篇论文\\2025.11.25-new work\\4.COPE\"\n",
    "    os.makedirs(work_dir, exist_ok=True)\n",
    "    \n",
    "    # 文件配置\n",
    "    train_file = \"训练集-量子计算.xlsx\"\n",
    "    ext_file = \"外部验证集-量子计算.xlsx\"\n",
    "    \n",
    "    train_path = os.path.join(work_dir, train_file)\n",
    "    ext_path = os.path.join(work_dir, ext_file)\n",
    "    \n",
    "    if not os.path.exists(train_path) or not os.path.exists(ext_path):\n",
    "        print(\"Error: Input files missing.\")\n",
    "        exit()\n",
    "        \n",
    "    # 1. 加载数据\n",
    "    print(\"Loading data...\")\n",
    "    d_train = pd.read_excel(train_path)\n",
    "    d_ext = pd.read_excel(ext_path)\n",
    "    \n",
    "    X_int = d_train.drop(columns=['Prognosis'])\n",
    "    y_int = d_train['Prognosis']\n",
    "    \n",
    "    X_ext = d_ext.drop(columns=['Prognosis'])\n",
    "    y_ext = d_ext['Prognosis']\n",
    "    \n",
    "    # 内部再次划分 Train/Test (用于训练基础模型 vs 计算权重)\n",
    "    # 这里的 X_test_int 将作为 \"Internal Validation\" 用来计算权重\n",
    "    X_train_base, X_test_int, y_train_base, y_test_int = train_test_split(\n",
    "        X_int, y_int, test_size=0.3, stratify=y_int, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 2. 训练基础模型\n",
    "    print(\"Training base models...\")\n",
    "    trained_models = train_base_models(X_train_base, y_train_base, classifiers)\n",
    "    \n",
    "    # 3. 运行动态阈值分析\n",
    "    # 使用 X_test_int 计算权重，然后应用到 X_ext\n",
    "    print(\"Starting dynamic threshold analysis...\")\n",
    "    run_dynamic_threshold_analysis(trained_models, X_test_int, y_test_int, X_ext, y_ext, work_dir)\n",
    "    \n",
    "    print(f\"\\nAnalysis complete. Results saved to {work_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc38d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# ================= 配置区域 =================\n",
    "work_dir = r\"D:\\卒中预后第一篇论文\\2025.11.25-new work\\4.COPE\"\n",
    "data_file = os.path.join(work_dir, \"Dynamic_Threshold_Analysis.xlsx\")\n",
    "output_dir = os.path.join(work_dir, \"Threshold_Dynamics_Plots\")\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# ================= 绘图函数 =================\n",
    "\n",
    "def plot_metric_dynamics(df, metrics, prefix, title_suffix):\n",
    "    \"\"\"绘制指标随阈值变化的曲线\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # 定义线条样式和颜色\n",
    "    styles = ['-', '--', '-.', ':', '-']\n",
    "    colors = sns.color_palette(\"tab10\", len(metrics))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        col_name = f\"{prefix}_{metric}\"\n",
    "        if col_name in df.columns:\n",
    "            plt.plot(df['Threshold'], df[col_name], \n",
    "                     label=metric, color=colors[i], linestyle=styles[i%len(styles)], linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Threshold Probability', fontsize=14)\n",
    "    plt.ylabel('Score', fontsize=14)\n",
    "    plt.title(f'COPE Performance vs Threshold ({title_suffix})', fontsize=16, fontweight='bold')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.ylim([0, 1.05])\n",
    "    \n",
    "    save_path = os.path.join(output_dir, f\"{prefix}_Metrics_Dynamics.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    # 保存PDF版本\n",
    "    plt.savefig(save_path.replace('.png', '.pdf'))\n",
    "    plt.close()\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "def plot_weight_dynamics_stacked(df):\n",
    "    \"\"\"绘制权重随阈值变化的堆叠面积图\"\"\"\n",
    "    weight_cols = [c for c in df.columns if c.startswith('Weight_')]\n",
    "    labels = [c.replace('Weight_', '') for c in weight_cols]\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # 绘制堆叠图\n",
    "    plt.stackplot(df['Threshold'], df[weight_cols].T, labels=labels, alpha=0.85, cmap='viridis')\n",
    "    \n",
    "    plt.xlabel('Threshold Probability', fontsize=14)\n",
    "    plt.ylabel('CRITIC Weight Contribution', fontsize=14)\n",
    "    plt.title('Dynamic Model Weights Allocation vs Threshold', fontsize=16, fontweight='bold')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3, fontsize=11)\n",
    "    plt.xlim([df['Threshold'].min(), df['Threshold'].max()])\n",
    "    plt.ylim([0, 1.0])\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.4)\n",
    "    \n",
    "    save_path = os.path.join(output_dir, \"Weights_Dynamics_Stacked.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.savefig(save_path.replace('.png', '.pdf'))\n",
    "    plt.close()\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "def plot_tradeoff_curves(df, prefix, title_suffix):\n",
    "    \"\"\"专门绘制敏感度-特异度权衡图\"\"\"\n",
    "    if f'{prefix}_SEN' not in df.columns or f'{prefix}_SPE' not in df.columns:\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    plt.plot(df['Threshold'], df[f'{prefix}_SEN'], label='Sensitivity', color='blue', lw=2)\n",
    "    plt.plot(df['Threshold'], df[f'{prefix}_SPE'], label='Specificity', color='green', lw=2)\n",
    "    \n",
    "    # 寻找交叉点（平衡点）\n",
    "    idx = np.argmin(np.abs(df[f'{prefix}_SEN'] - df[f'{prefix}_SPE']))\n",
    "    cross_thresh = df.iloc[idx]['Threshold']\n",
    "    cross_val = df.iloc[idx][f'{prefix}_SEN']\n",
    "    \n",
    "    plt.scatter(cross_thresh, cross_val, color='red', zorder=5)\n",
    "    plt.annotate(f'Balance: {cross_thresh:.2f}', (cross_thresh, cross_val), \n",
    "                 xytext=(10, 10), textcoords='offset points', fontsize=10)\n",
    "\n",
    "    plt.xlabel('Threshold', fontsize=14)\n",
    "    plt.ylabel('Score', fontsize=14)\n",
    "    plt.title(f'Sensitivity vs Specificity Trade-off ({title_suffix})', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    \n",
    "    save_path = os.path.join(output_dir, f\"{prefix}_Tradeoff.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.savefig(save_path.replace('.png', '.pdf'))\n",
    "    plt.close()\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "# ================= 主程序 =================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(data_file):\n",
    "        print(f\"Error: File not found at {data_file}\")\n",
    "        print(\"Please run the previous analysis code first to generate 'Dynamic_Threshold_Analysis.xlsx'.\")\n",
    "        exit()\n",
    "        \n",
    "    print(f\"Loading data from: {data_file}\")\n",
    "    df = pd.read_excel(data_file)\n",
    "    \n",
    "    # 1. 绘制内部验证集指标动态\n",
    "    metrics_to_plot = ['AUC', 'F1', 'ACC', 'SEN', 'SPE', 'PPV', 'NPV']\n",
    "    # 筛选存在的列\n",
    "    metrics_int = [m for m in metrics_to_plot if f'Int_{m}' in df.columns]\n",
    "    plot_metric_dynamics(df, metrics_int, 'Int', 'Internal Validation')\n",
    "    \n",
    "    # 2. 绘制外部验证集指标动态\n",
    "    metrics_ext = [m for m in metrics_to_plot if f'Ext_{m}' in df.columns]\n",
    "    plot_metric_dynamics(df, metrics_ext, 'Ext', 'External Validation')\n",
    "    \n",
    "    # 3. 绘制权重堆叠图\n",
    "    plot_weight_dynamics_stacked(df)\n",
    "    \n",
    "    # 4. 绘制敏感度-特异度权衡图\n",
    "    plot_tradeoff_curves(df, 'Int', 'Internal')\n",
    "    plot_tradeoff_curves(df, 'Ext', 'External')\n",
    "    \n",
    "    print(\"\\nAll plots generated successfully in 'Threshold_Dynamics_Plots' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Sklearn & Models\n",
    "from sklearn.metrics import (roc_curve, auc, roc_auc_score, f1_score, accuracy_score, \n",
    "                             precision_score, recall_score, confusion_matrix, brier_score_loss)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Boosting Libraries\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "# ======================== 1. 基础计算与指标函数 ========================\n",
    "\n",
    "def bootstrap_ci(y_true, y_prob, metric_func, n_bootstraps=1000, alpha=0.05, **kwargs):\n",
    "    \"\"\"Bootstrap计算95% CI\"\"\"\n",
    "    rng = np.random.RandomState(42)\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(n_bootstraps):\n",
    "        indices = rng.randint(0, len(y_prob), len(y_prob))\n",
    "        if len(np.unique(y_true[indices])) < 2: continue\n",
    "        try:\n",
    "            scores.append(metric_func(y_true[indices], y_prob[indices], **kwargs))\n",
    "        except: continue\n",
    "            \n",
    "    if not scores: return np.nan, np.nan\n",
    "    scores = np.sort(scores)\n",
    "    return np.percentile(scores, alpha/2*100), np.percentile(scores, (1-alpha/2)*100)\n",
    "\n",
    "def calculate_metrics(y_true, y_prob, threshold):\n",
    "    \"\"\"计算指定阈值下的单次指标 (不含CI，用于循环中快速计算权重)\"\"\"\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sen = recall_score(y_true, y_pred, zero_division=0)\n",
    "    spe = tn / (tn + fp + 1e-10)\n",
    "    ppv = precision_score(y_true, y_pred, zero_division=0)\n",
    "    npv = tn / (tn + fn + 1e-10)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # AUC 和 Brier 与阈值无关，但在表中展示作为参考\n",
    "    auc_val = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    \n",
    "    return {\n",
    "        'AUC': auc_val, 'F1': f1, 'ACC': acc, \n",
    "        'SEN': sen, 'SPE': spe, 'PPV': ppv, 'NPV': npv\n",
    "    }\n",
    "\n",
    "def get_full_metrics_with_ci(y_true, y_prob, threshold):\n",
    "    \"\"\"计算完整指标含CI (用于最终报告)\"\"\"\n",
    "    base = calculate_metrics(y_true, y_prob, threshold)\n",
    "    \n",
    "    # 补充 Brier\n",
    "    base['Brier'] = brier_score_loss(y_true, y_prob)\n",
    "    \n",
    "    # 计算 AUC CI\n",
    "    low, high = bootstrap_ci(y_true, y_prob, roc_auc_score, n_bootstraps=500)\n",
    "    base['AUC_95CI'] = f\"{base['AUC']:.3f} ({low:.3f}-{high:.3f})\"\n",
    "    \n",
    "    # 计算 Brier CI\n",
    "    b_low, b_high = bootstrap_ci(y_true, y_prob, brier_score_loss, n_bootstraps=500)\n",
    "    base['Brier_95CI'] = f\"{base['Brier']:.3f} ({b_low:.3f}-{b_high:.3f})\"\n",
    "    \n",
    "    return base\n",
    "\n",
    "# ======================== 2. CRITIC 权重计算 ========================\n",
    "\n",
    "def critic_weighting(metrics_df):\n",
    "    \"\"\"\n",
    "    基于当前指标数据框计算 CRITIC 权重\n",
    "    metrics_df: 行是模型，列是指标 (AUC, F1, ACC, SEN, SPE)\n",
    "    \"\"\"\n",
    "    df = metrics_df.copy()\n",
    "    # 选取用于权重的指标 (正向指标)\n",
    "    indicators = ['AUC', 'F1', 'ACC', 'SEN', 'SPE']\n",
    "    valid_inds = [c for c in indicators if c in df.columns]\n",
    "    \n",
    "    # 获取数据矩阵\n",
    "    Z = df[valid_inds].values\n",
    "    \n",
    "    # 极差归一化\n",
    "    ptp = Z.max(axis=0) - Z.min(axis=0)\n",
    "    ptp[ptp == 0] = 1e-8 # 防止除零\n",
    "    Z_norm = (Z - Z.min(axis=0)) / ptp\n",
    "    \n",
    "    # 标准差 (对比强度)\n",
    "    std_dev = np.std(Z_norm, axis=0)\n",
    "    \n",
    "    # 相关系数 (冲突性)\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        corr_matrix = np.corrcoef(Z_norm, rowvar=False)\n",
    "    if np.isnan(corr_matrix).any(): \n",
    "        # 如果某列全是0(如阈值极端时)，相关系数无法计算，设冲突系数为1(无冲突信息)\n",
    "        conflict = np.ones(len(valid_inds))\n",
    "    else:\n",
    "        conflict = np.sum(1 - np.abs(corr_matrix), axis=1)\n",
    "    \n",
    "    # 信息量\n",
    "    info = std_dev * conflict\n",
    "    \n",
    "    # 指标权重\n",
    "    w_j = info / (np.sum(info) + 1e-10)\n",
    "    \n",
    "    # 模型综合得分\n",
    "    model_scores = np.sum(Z_norm * w_j, axis=1)\n",
    "    \n",
    "    # 归一化为模型权重\n",
    "    final_model_weights = model_scores / (np.sum(model_scores) + 1e-10)\n",
    "    \n",
    "    return dict(zip(df['Classifier'], final_model_weights))\n",
    "\n",
    "def get_ensemble_probs(probs_dict, weights):\n",
    "    \"\"\"根据权重组合概率\"\"\"\n",
    "    ensemble_probs = np.zeros_like(list(probs_dict.values())[0])\n",
    "    total_weight = 0\n",
    "    for name, w in weights.items():\n",
    "        if name in probs_dict:\n",
    "            ensemble_probs += w * probs_dict[name]\n",
    "            total_weight += w\n",
    "    return ensemble_probs / (total_weight + 1e-10)\n",
    "\n",
    "# ======================== 3. 绘图函数 ========================\n",
    "\n",
    "def plot_roc_curves(y_true, probs_dict, save_path, title_suffix=\"\"):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(probs_dict)))\n",
    "    \n",
    "    for i, (name, probs) in enumerate(probs_dict.items()):\n",
    "        fpr, tpr, _ = roc_curve(y_true, probs)\n",
    "        auc_val = roc_auc_score(y_true, probs)\n",
    "        plt.plot(fpr, tpr, color=colors[i], lw=2, label=f'{name} (AUC={auc_val:.3f})')\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves - {title_suffix}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(save_path.replace('.png', '.pdf'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_calibration(y_true, probs_dict, save_path, title_suffix=\"\"):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(probs_dict)))\n",
    "    \n",
    "    for i, (name, probs) in enumerate(probs_dict.items()):\n",
    "        frac, mean_val = calibration_curve(y_true, probs, n_bins=10)\n",
    "        brier = brier_score_loss(y_true, probs)\n",
    "        plt.plot(mean_val, frac, \"s-\", color=colors[i], label=f\"{name} (Brier: {brier:.3f})\")\n",
    "    \n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title(f'Calibration - {title_suffix}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(save_path.replace('.png', '.pdf'))\n",
    "    plt.close()\n",
    "\n",
    "# ======================== 4. 主逻辑流程 ========================\n",
    "\n",
    "def train_base_models(X_train, y_train, classifiers):\n",
    "    models = {}\n",
    "    for name, clf in classifiers:\n",
    "        print(f\"  Training {name}...\")\n",
    "        model = clone(clf)\n",
    "        model.fit(X_train, y_train)\n",
    "        models[name] = model\n",
    "    return models\n",
    "\n",
    "def run_dynamic_threshold_analysis(models, X_int, y_int, X_ext, y_ext, save_dir):\n",
    "    \"\"\"\n",
    "    核心函数：遍历阈值 -> 动态计算权重 -> 动态构建COPE -> 评估内外部\n",
    "    \"\"\"\n",
    "    # 1. 预先计算所有基础模型的概率 (避免循环中重复预测)\n",
    "    base_probs_int = {}\n",
    "    base_probs_ext = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            base_probs_int[name] = model.predict_proba(X_int)[:, 1]\n",
    "            base_probs_ext[name] = model.predict_proba(X_ext)[:, 1]\n",
    "        else:\n",
    "            base_probs_int[name] = model.predict(X_int)\n",
    "            base_probs_ext[name] = model.predict(X_ext)\n",
    "            \n",
    "    # 2. 遍历阈值\n",
    "    thresholds = np.arange(0.05, 0.96, 0.05)\n",
    "    dynamic_results = []\n",
    "    best_f1 = -1\n",
    "    best_threshold = 0.5\n",
    "    best_weights = None\n",
    "    \n",
    "    print(\"  Running dynamic threshold loop...\")\n",
    "    for thresh in thresholds:\n",
    "        thresh = round(thresh, 2)\n",
    "        \n",
    "        # A. 计算内部集上基础模型的指标 (用于权重)\n",
    "        model_metrics_list = []\n",
    "        for name in models.keys():\n",
    "            m = calculate_metrics(y_int, base_probs_int[name], thresh)\n",
    "            m['Classifier'] = name\n",
    "            model_metrics_list.append(m)\n",
    "        \n",
    "        metrics_df = pd.DataFrame(model_metrics_list)\n",
    "        \n",
    "        # B. 动态计算 CRITIC 权重\n",
    "        current_weights = critic_weighting(metrics_df)\n",
    "        \n",
    "        # C. 构建动态 COPE\n",
    "        cope_prob_int = get_ensemble_probs(base_probs_int, current_weights)\n",
    "        cope_prob_ext = get_ensemble_probs(base_probs_ext, current_weights)\n",
    "        \n",
    "        # D. 评估 COPE (内部 & 外部)\n",
    "        cope_res_int = calculate_metrics(y_int, cope_prob_int, thresh)\n",
    "        cope_res_ext = calculate_metrics(y_ext, cope_prob_ext, thresh)\n",
    "        \n",
    "        # 记录结果\n",
    "        row = {'Threshold': thresh}\n",
    "        # 记录权重\n",
    "        for k, v in current_weights.items():\n",
    "            row[f'Weight_{k}'] = v\n",
    "        # 记录内部指标\n",
    "        for k, v in cope_res_int.items():\n",
    "            row[f'Int_{k}'] = v\n",
    "        # 记录外部指标\n",
    "        for k, v in cope_res_ext.items():\n",
    "            row[f'Ext_{k}'] = v\n",
    "            \n",
    "        dynamic_results.append(row)\n",
    "        \n",
    "        # 记录最佳阈值 (以内部 F1 为准)\n",
    "        if cope_res_int['F1'] > best_f1:\n",
    "            best_f1 = cope_res_int['F1']\n",
    "            best_threshold = thresh\n",
    "            best_weights = current_weights\n",
    "            \n",
    "    # 3. 保存动态分析大表\n",
    "    analysis_df = pd.DataFrame(dynamic_results)\n",
    "    analysis_df.to_excel(os.path.join(save_dir, 'Dynamic_Threshold_Analysis.xlsx'), index=False)\n",
    "    \n",
    "    # 4. 基于最佳阈值生成最终详细报告和图表\n",
    "    print(f\"  Best Threshold found: {best_threshold} (Internal F1: {best_f1:.3f})\")\n",
    "    \n",
    "    # 构建最佳 COPE 概率\n",
    "    final_cope_prob_int = get_ensemble_probs(base_probs_int, best_weights)\n",
    "    final_cope_prob_ext = get_ensemble_probs(base_probs_ext, best_weights)\n",
    "    \n",
    "    # 准备绘图数据 (加入 COPE)\n",
    "    plot_probs_int = base_probs_int.copy()\n",
    "    plot_probs_int['COPE'] = final_cope_prob_int\n",
    "    \n",
    "    plot_probs_ext = base_probs_ext.copy()\n",
    "    plot_probs_ext['COPE'] = final_cope_prob_ext\n",
    "    \n",
    "    # 生成图表\n",
    "    plot_roc_curves(y_int, plot_probs_int, os.path.join(save_dir, 'Internal_ROC.png'), \"Internal\")\n",
    "    plot_roc_curves(y_ext, plot_probs_ext, os.path.join(save_dir, 'External_ROC.png'), \"External\")\n",
    "    \n",
    "    plot_calibration(y_int, plot_probs_int, os.path.join(save_dir, 'Internal_Calibration.png'), \"Internal\")\n",
    "    plot_calibration(y_ext, plot_probs_ext, os.path.join(save_dir, 'External_Calibration.png'), \"External\")\n",
    "    \n",
    "    # 生成详细指标表 (含CI)\n",
    "    final_metrics = []\n",
    "    # 内部\n",
    "    for name, p in plot_probs_int.items():\n",
    "        m = get_full_metrics_with_ci(y_int, p, best_threshold)\n",
    "        m['Classifier'] = name\n",
    "        m['Dataset'] = 'Internal'\n",
    "        final_metrics.append(m)\n",
    "    # 外部\n",
    "    for name, p in plot_probs_ext.items():\n",
    "        m = get_full_metrics_with_ci(y_ext, p, best_threshold)\n",
    "        m['Classifier'] = name\n",
    "        m['Dataset'] = 'External'\n",
    "        final_metrics.append(m)\n",
    "        \n",
    "    pd.DataFrame(final_metrics).to_excel(os.path.join(save_dir, 'Final_Optimal_Metrics.xlsx'), index=False)\n",
    "    \n",
    "    # 绘制权重变化图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    weight_cols = [c for c in analysis_df.columns if c.startswith('Weight_')]\n",
    "    for col in weight_cols:\n",
    "        plt.plot(analysis_df['Threshold'], analysis_df[col], label=col.replace('Weight_', ''))\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('CRITIC Weight')\n",
    "    plt.title('Dynamic CRITIC Weights vs Threshold')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'Weights_Dynamics.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# ======================== 5. 执行入口 ========================\n",
    "\n",
    "classifiers = [\n",
    "    ('Extra Trees', ExtraTreesClassifier(n_estimators=100, random_state=42)),\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ('LightGBM', lgb.LGBMClassifier(random_state=42, verbose=-1)),\n",
    "    ('CatBoost', cb.CatBoostClassifier(random_state=42, verbose=0, allow_writing_files=False)),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    work_dir = r\"D:/卒中预后第二篇论文/2025.11.25-new work/4.1 Non-CPD COPE\"\n",
    "    os.makedirs(work_dir, exist_ok=True)\n",
    "    \n",
    "    # 文件配置\n",
    "    train_file = \"训练集-量子计算.xlsx\"\n",
    "    ext_file = \"外部验证集-量子计算.xlsx\"\n",
    "    \n",
    "    train_path = os.path.join(work_dir, train_file)\n",
    "    ext_path = os.path.join(work_dir, ext_file)\n",
    "    \n",
    "    if not os.path.exists(train_path) or not os.path.exists(ext_path):\n",
    "        print(\"Error: Input files missing.\")\n",
    "        exit()\n",
    "        \n",
    "    # 1. 加载数据\n",
    "    print(\"Loading data...\")\n",
    "    d_train = pd.read_excel(train_path)\n",
    "    d_ext = pd.read_excel(ext_path)\n",
    "    \n",
    "    X_int = d_train.drop(columns=['Prognosis'])\n",
    "    y_int = d_train['Prognosis']\n",
    "    \n",
    "    X_ext = d_ext.drop(columns=['Prognosis'])\n",
    "    y_ext = d_ext['Prognosis']\n",
    "    \n",
    "    # 内部再次划分 Train/Test (用于训练基础模型 vs 计算权重)\n",
    "    # 这里的 X_test_int 将作为 \"Internal Validation\" 用来计算权重\n",
    "    X_train_base, X_test_int, y_train_base, y_test_int = train_test_split(\n",
    "        X_int, y_int, test_size=0.3, stratify=y_int, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 2. 训练基础模型\n",
    "    print(\"Training base models...\")\n",
    "    trained_models = train_base_models(X_train_base, y_train_base, classifiers)\n",
    "    \n",
    "    # 3. 运行动态阈值分析\n",
    "    # 使用 X_test_int 计算权重，然后应用到 X_ext\n",
    "    print(\"Starting dynamic threshold analysis...\")\n",
    "    run_dynamic_threshold_analysis(trained_models, X_test_int, y_test_int, X_ext, y_ext, work_dir)\n",
    "    \n",
    "    print(f\"\\nAnalysis complete. Results saved to {work_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# ================= 配置区域 =================\n",
    "work_dir = r\"D:/卒中预后第二篇论文/2025.11.25-new work/4.1 Non-CPD COPE\"\n",
    "data_file = os.path.join(work_dir, \"Dynamic_Threshold_Analysis.xlsx\")\n",
    "output_dir = os.path.join(work_dir, \"Threshold_Dynamics_Plots\")\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# ================= 绘图函数 =================\n",
    "\n",
    "def plot_metric_dynamics(df, metrics, prefix, title_suffix):\n",
    "    \"\"\"绘制指标随阈值变化的曲线\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # 定义线条样式和颜色\n",
    "    styles = ['-', '--', '-.', ':', '-']\n",
    "    colors = sns.color_palette(\"tab10\", len(metrics))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        col_name = f\"{prefix}_{metric}\"\n",
    "        if col_name in df.columns:\n",
    "            plt.plot(df['Threshold'], df[col_name], \n",
    "                     label=metric, color=colors[i], linestyle=styles[i%len(styles)], linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Threshold Probability', fontsize=14)\n",
    "    plt.ylabel('Score', fontsize=14)\n",
    "    plt.title(f'COPE Performance vs Threshold ({title_suffix})', fontsize=16, fontweight='bold')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.ylim([0, 1.05])\n",
    "    \n",
    "    save_path = os.path.join(output_dir, f\"{prefix}_Metrics_Dynamics.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    # 保存PDF版本\n",
    "    plt.savefig(save_path.replace('.png', '.pdf'))\n",
    "    plt.close()\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "def plot_weight_dynamics_stacked(df):\n",
    "    \"\"\"绘制权重随阈值变化的堆叠面积图\"\"\"\n",
    "    weight_cols = [c for c in df.columns if c.startswith('Weight_')]\n",
    "    labels = [c.replace('Weight_', '') for c in weight_cols]\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # 绘制堆叠图\n",
    "    plt.stackplot(df['Threshold'], df[weight_cols].T, labels=labels, alpha=0.85, cmap='viridis')\n",
    "    \n",
    "    plt.xlabel('Threshold Probability', fontsize=14)\n",
    "    plt.ylabel('CRITIC Weight Contribution', fontsize=14)\n",
    "    plt.title('Dynamic Model Weights Allocation vs Threshold', fontsize=16, fontweight='bold')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3, fontsize=11)\n",
    "    plt.xlim([df['Threshold'].min(), df['Threshold'].max()])\n",
    "    plt.ylim([0, 1.0])\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.4)\n",
    "    \n",
    "    save_path = os.path.join(output_dir, \"Weights_Dynamics_Stacked.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.savefig(save_path.replace('.png', '.pdf'))\n",
    "    plt.close()\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "def plot_tradeoff_curves(df, prefix, title_suffix):\n",
    "    \"\"\"专门绘制敏感度-特异度权衡图\"\"\"\n",
    "    if f'{prefix}_SEN' not in df.columns or f'{prefix}_SPE' not in df.columns:\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    plt.plot(df['Threshold'], df[f'{prefix}_SEN'], label='Sensitivity', color='blue', lw=2)\n",
    "    plt.plot(df['Threshold'], df[f'{prefix}_SPE'], label='Specificity', color='green', lw=2)\n",
    "    \n",
    "    # 寻找交叉点（平衡点）\n",
    "    idx = np.argmin(np.abs(df[f'{prefix}_SEN'] - df[f'{prefix}_SPE']))\n",
    "    cross_thresh = df.iloc[idx]['Threshold']\n",
    "    cross_val = df.iloc[idx][f'{prefix}_SEN']\n",
    "    \n",
    "    plt.scatter(cross_thresh, cross_val, color='red', zorder=5)\n",
    "    plt.annotate(f'Balance: {cross_thresh:.2f}', (cross_thresh, cross_val), \n",
    "                 xytext=(10, 10), textcoords='offset points', fontsize=10)\n",
    "\n",
    "    plt.xlabel('Threshold', fontsize=14)\n",
    "    plt.ylabel('Score', fontsize=14)\n",
    "    plt.title(f'Sensitivity vs Specificity Trade-off ({title_suffix})', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    \n",
    "    save_path = os.path.join(output_dir, f\"{prefix}_Tradeoff.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.savefig(save_path.replace('.png', '.pdf'))\n",
    "    plt.close()\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "# ================= 主程序 =================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(data_file):\n",
    "        print(f\"Error: File not found at {data_file}\")\n",
    "        print(\"Please run the previous analysis code first to generate 'Dynamic_Threshold_Analysis.xlsx'.\")\n",
    "        exit()\n",
    "        \n",
    "    print(f\"Loading data from: {data_file}\")\n",
    "    df = pd.read_excel(data_file)\n",
    "    \n",
    "    # 1. 绘制内部验证集指标动态\n",
    "    metrics_to_plot = ['AUC', 'F1', 'ACC', 'SEN', 'SPE', 'PPV', 'NPV']\n",
    "    # 筛选存在的列\n",
    "    metrics_int = [m for m in metrics_to_plot if f'Int_{m}' in df.columns]\n",
    "    plot_metric_dynamics(df, metrics_int, 'Int', 'Internal Validation')\n",
    "    \n",
    "    # 2. 绘制外部验证集指标动态\n",
    "    metrics_ext = [m for m in metrics_to_plot if f'Ext_{m}' in df.columns]\n",
    "    plot_metric_dynamics(df, metrics_ext, 'Ext', 'External Validation')\n",
    "    \n",
    "    # 3. 绘制权重堆叠图\n",
    "    plot_weight_dynamics_stacked(df)\n",
    "    \n",
    "    # 4. 绘制敏感度-特异度权衡图\n",
    "    plot_tradeoff_curves(df, 'Int', 'Internal')\n",
    "    plot_tradeoff_curves(df, 'Ext', 'External')\n",
    "    \n",
    "    print(\"\\nAll plots generated successfully in 'Threshold_Dynamics_Plots' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd93284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib\n",
    "\n",
    "# ======================== 修改点 1: 设置 PDF 字体为 Type 42 (可编辑) ========================\n",
    "matplotlib.use('Agg')  # 非交互式后端\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "# ==========================================================================================\n",
    "\n",
    "# Sklearn & Models\n",
    "from sklearn.metrics import (roc_auc_score, f1_score, accuracy_score, precision_score, \n",
    "                             recall_score, confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置绘图字体\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ======================== 1. 基础训练与权重计算函数 ========================\n",
    "\n",
    "def train_base_models(X_train, y_train, classifiers):\n",
    "    \"\"\"训练基础模型\"\"\"\n",
    "    trained_models = {}\n",
    "    for name, clf in classifiers:\n",
    "        try:\n",
    "            model = clone(clf)\n",
    "            model.fit(X_train, y_train)\n",
    "            trained_models[name] = model\n",
    "            print(f\"  Trained {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to train {name}: {e}\")\n",
    "    return trained_models\n",
    "\n",
    "def get_metrics(y_true, y_prob):\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return {\n",
    "        'AUC': roc_auc_score(y_true, y_prob),\n",
    "        'F1': f1_score(y_true, y_pred),\n",
    "        'ACC': accuracy_score(y_true, y_pred),\n",
    "        'SEN': recall_score(y_true, y_pred),\n",
    "        'SPE': tn / (tn + fp + 1e-10)\n",
    "    }\n",
    "\n",
    "def critic_weighting(metrics_list):\n",
    "    \"\"\"CRITIC 权重计算\"\"\"\n",
    "    df = pd.DataFrame(metrics_list)\n",
    "    indicators = ['AUC', 'F1', 'ACC', 'SEN', 'SPE']\n",
    "    Z = df[indicators].values\n",
    "    \n",
    "    # 归一化\n",
    "    Z_norm = (Z - Z.min(axis=0)) / (Z.max(axis=0) - Z.min(axis=0) + 1e-8)\n",
    "    \n",
    "    std_dev = np.std(Z_norm, axis=0)\n",
    "    corr_matrix = np.corrcoef(Z_norm, rowvar=False)\n",
    "    if np.isnan(corr_matrix).any():\n",
    "        conflict = np.ones(len(indicators))\n",
    "    else:\n",
    "        conflict = np.sum(1 - np.abs(corr_matrix), axis=1)\n",
    "        \n",
    "    info = std_dev * conflict\n",
    "    weights = info / (np.sum(info) + 1e-10)\n",
    "    \n",
    "    model_scores = np.sum(Z_norm * weights, axis=1)\n",
    "    final_weights = model_scores / (np.sum(model_scores) + 1e-10)\n",
    "    \n",
    "    return dict(zip(df['Classifier'], final_weights))\n",
    "\n",
    "# ======================== 2. SHAP 计算函数 ========================\n",
    "\n",
    "def compute_shap_for_model(model, X_train, X_test):\n",
    "    \"\"\"计算单个模型的 SHAP 值\"\"\"\n",
    "    # 降采样背景数据加速 KernelExplainer\n",
    "    background = shap.sample(X_train, 50) \n",
    "    \n",
    "    if isinstance(model, (lgb.LGBMClassifier, cb.CatBoostClassifier, ExtraTreesClassifier)):\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "    else:\n",
    "        # Logistic Regression, AdaBoost 等用 KernelExplainer\n",
    "        # 使用 predict_proba 解释概率输出\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, background)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "    # 处理 SHAP 输出格式差异\n",
    "    if isinstance(shap_values, list):\n",
    "        # Binary classification, take positive class (index 1)\n",
    "        if len(shap_values) == 2:\n",
    "            return shap_values[1]\n",
    "        return shap_values[0]\n",
    "    elif len(np.shape(shap_values)) == 3:\n",
    "        # (samples, features, classes) -> take class 1\n",
    "        return shap_values[:, :, 1]\n",
    "    \n",
    "    return shap_values\n",
    "\n",
    "def compute_cope_shap(trained_models, weights, X_train, X_test):\n",
    "    \"\"\"计算加权集成的 COPE SHAP 值\"\"\"\n",
    "    print(\"  Computing SHAP for ensemble...\")\n",
    "    \n",
    "    # 初始化\n",
    "    n_samples, n_features = X_test.shape\n",
    "    cope_shap = np.zeros((n_samples, n_features))\n",
    "    \n",
    "    for name, model in trained_models.items():\n",
    "        if name not in weights: continue\n",
    "        w = weights[name]\n",
    "        \n",
    "        try:\n",
    "            sv = compute_shap_for_model(model, X_train, X_test)\n",
    "            # 确保形状匹配\n",
    "            if sv.shape == cope_shap.shape:\n",
    "                cope_shap += w * sv\n",
    "            else:\n",
    "                print(f\"    Shape mismatch for {name}, skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error computing SHAP for {name}: {e}\")\n",
    "            \n",
    "    return cope_shap\n",
    "\n",
    "# ======================== 3. 不同阈值下的可视化函数 ========================\n",
    "\n",
    "def plot_shap_by_threshold_groups(shap_values, X_test, probs, save_dir):\n",
    "    \"\"\"\n",
    "    将样本按预测概率分为低、中、高风险组，分别绘制 SHAP Summary Plot\n",
    "    \"\"\"\n",
    "    groups = {\n",
    "        'Low_Risk_Only (Prob < 0.3)': probs < 0.3,\n",
    "        'Medium_Risk_Only (0.3 <= Prob <= 0.6)': (probs >= 0.3) & (probs <= 0.6),\n",
    "        'High_Risk_Only (Prob > 0.6)': probs > 0.6\n",
    "    }\n",
    "    \n",
    "    for label, mask in groups.items():\n",
    "        if np.sum(mask) < 5:\n",
    "            print(f\"    Skipping {label}: not enough samples ({np.sum(mask)})\")\n",
    "            continue\n",
    "            \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values[mask], X_test[mask], show=False)\n",
    "        plt.title(f\"SHAP Summary - {label}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, f\"SHAP_Summary_{label.split()[0]}.pdf\"))\n",
    "        plt.close()\n",
    "\n",
    "def plot_feature_importance_heatmap_by_threshold(shap_values, X_test, probs, feature_names, save_dir):\n",
    "    \"\"\"\n",
    "    绘制热图：不同风险区间下，各特征的平均绝对SHAP值\n",
    "    \"\"\"\n",
    "    bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    labels = ['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']\n",
    "    \n",
    "    prob_bins = pd.cut(probs, bins=bins, labels=labels)\n",
    "    \n",
    "    df_shap = pd.DataFrame(np.abs(shap_values), columns=feature_names)\n",
    "    df_shap['Bin'] = prob_bins\n",
    "    \n",
    "    grouped = df_shap.groupby('Bin').mean()\n",
    "    \n",
    "    # 仅选取总重要性排名前 10 的特征\n",
    "    top_features = df_shap.drop('Bin', axis=1).mean().sort_values(ascending=False).head(10).index\n",
    "    plot_data = grouped[top_features].T\n",
    "    \n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.heatmap(plot_data, cmap='viridis', annot=True, fmt='.3f', linewidths=.5)\n",
    "    plt.title(\"Mean |SHAP| Value by Risk Probability Interval\", fontsize=14)\n",
    "    plt.xlabel(\"Predicted Probability Interval\", fontsize=12)\n",
    "    plt.ylabel(\"Top 10 Features\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"SHAP_Feature_Importance_Heatmap_by_Threshold.pdf\"))\n",
    "    plt.close()\n",
    "\n",
    "# ======================== 修改点 2: 合并 Decision Plot 函数 ========================\n",
    "def plot_decision_plot_combined(shap_values, X_test, probs, save_dir):\n",
    "    \"\"\"\n",
    "    将低风险和高风险样本的 Decision Plot 合并到一张图中 (1x2 Subplots)\n",
    "    \"\"\"\n",
    "    # 按概率排序\n",
    "    sorted_idx = np.argsort(probs)\n",
    "    \n",
    "    # 选几个代表点\n",
    "    low_idx = sorted_idx[:20] # 最低概率的20个\n",
    "    high_idx = sorted_idx[-20:] # 最高概率的20个\n",
    "    \n",
    "    base_value = np.mean(probs)\n",
    "    \n",
    "    # 创建画布：宽20，高8，准备画左右两张图\n",
    "    fig = plt.figure(figsize=(20, 8))\n",
    "    \n",
    "    # --- 左图：Low Risk ---\n",
    "    # 使用 fig.add_subplot 激活左侧子图区域，shap.decision_plot 会绘制在当前激活的轴上\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    shap.decision_plot(\n",
    "        base_value, \n",
    "        shap_values[low_idx], \n",
    "        X_test.iloc[low_idx], \n",
    "        show=False, \n",
    "        link='logit'\n",
    "    )\n",
    "    # 为左子图添加标题 (注意：decision_plot可能会有些遮挡，调整y位置)\n",
    "    ax1.set_title(\"Decision Path - Lowest Probability Samples\", fontsize=16, y=1.02)\n",
    "    \n",
    "    # --- 右图：High Risk ---\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    shap.decision_plot(\n",
    "        base_value, \n",
    "        shap_values[high_idx], \n",
    "        X_test.iloc[high_idx], \n",
    "        show=False, \n",
    "        link='logit'\n",
    "    )\n",
    "    ax2.set_title(\"Decision Path - Highest Probability Samples\", fontsize=16, y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # 保存为一张合并的 PDF\n",
    "    plt.savefig(os.path.join(save_dir, \"Decision_Plot_Combined_Risk.pdf\"), bbox_inches='tight')\n",
    "    plt.close()\n",
    "# =================================================================================\n",
    "\n",
    "# ======================== 4. 主程序 ========================\n",
    "\n",
    "classifiers = [\n",
    "    ('Extra Trees', ExtraTreesClassifier(n_estimators=100, random_state=42)),\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ('LightGBM', lgb.LGBMClassifier(random_state=42, verbose=-1)),\n",
    "    ('CatBoost', cb.CatBoostClassifier(random_state=42, verbose=0, allow_writing_files=False)),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 设置路径\n",
    "    work_dir = r\"D:\\卒中预后第二篇论文\\2025.11.25-new work\\5.SHAP\"\n",
    "    data_path = os.path.join(work_dir, \"训练集-量子计算.xlsx\")\n",
    "    save_dir = os.path.join(work_dir, \"Threshold_SHAP_Results\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Error: Data file not found at {data_path}\")\n",
    "        exit()\n",
    "\n",
    "    # 2. 加载数据\n",
    "    print(\"Loading data...\")\n",
    "    data = pd.read_excel(data_path)\n",
    "    X = data.drop(columns=['Prognosis'])\n",
    "    y = data['Prognosis']\n",
    "    \n",
    "    # 处理非数值列 (简单编码)\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            X[col] = X[col].astype('category').cat.codes\n",
    "            \n",
    "    # 3. 划分训练/测试\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 4. 训练基础模型\n",
    "    print(\"Training base models...\")\n",
    "    models = train_base_models(X_train, y_train, classifiers)\n",
    "    \n",
    "    # 5. 计算 CRITIC 权重 (一次性计算全局权重用于 COPE)\n",
    "    print(\"Calculating CRITIC weights...\")\n",
    "    metrics_list = []\n",
    "    for name, model in models.items():\n",
    "        p = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(X_test)\n",
    "        m = get_metrics(y_test, p)\n",
    "        m['Classifier'] = name\n",
    "        metrics_list.append(m)\n",
    "    weights = critic_weighting(metrics_list)\n",
    "    \n",
    "    # 6. 计算 COPE 概率\n",
    "    print(\"Calculating COPE predictions...\")\n",
    "    cope_probs = np.zeros(len(X_test))\n",
    "    total_w = 0\n",
    "    for name, model in models.items():\n",
    "        w = weights[name]\n",
    "        p = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(X_test)\n",
    "        cope_probs += w * p\n",
    "        total_w += w\n",
    "    cope_probs /= total_w\n",
    "    \n",
    "    # 7. 计算 COPE SHAP (加权集成)\n",
    "    cope_shap = compute_cope_shap(models, weights, X_train, X_test)\n",
    "    \n",
    "    # 8. 执行不同阈值下的 SHAP 可视化\n",
    "    print(\"Generating threshold-stratified SHAP plots...\")\n",
    "    \n",
    "    # A. 分组 Summary Plot (Low/Medium/High Risk)\n",
    "    plot_shap_by_threshold_groups(cope_shap, X_test, cope_probs, save_dir)\n",
    "    \n",
    "    # B. 特征重要性热图 (随概率区间变化)\n",
    "    plot_feature_importance_heatmap_by_threshold(cope_shap, X_test, cope_probs, X.columns, save_dir)\n",
    "    \n",
    "    # C. 决策路径图 (已修改为合并图)\n",
    "    print(\"  Creating Combined Decision Plot...\")\n",
    "    plot_decision_plot_combined(cope_shap, X_test, cope_probs, save_dir)\n",
    "    \n",
    "    # D. 保存全局 SHAP Summary 作为参考\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(cope_shap, X_test, show=False)\n",
    "    plt.title(\"Global COPE SHAP Summary\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"SHAP_Summary_Global.pdf\"))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nAnalysis complete. All plots saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering, Birch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================= 设置绘图风格 =================\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['pdf.fonttype'] = 42 # 确保导出为矢量字体\n",
    "\n",
    "class StrokeSubphenotypeClustering:\n",
    "    def __init__(self, work_dir, n_clusters=2, random_state=42):\n",
    "        self.work_dir = work_dir\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        self.models = {\n",
    "            'GMM': GaussianMixture(n_components=n_clusters, random_state=random_state),\n",
    "            'KMeans': KMeans(n_clusters=n_clusters, random_state=random_state),\n",
    "            'MiniBatchKMeans': MiniBatchKMeans(n_clusters=n_clusters, random_state=random_state),\n",
    "            'HAC': AgglomerativeClustering(n_clusters=n_clusters),\n",
    "            'Birch': Birch(n_clusters=n_clusters)\n",
    "        }\n",
    "        \n",
    "        self.output_dir = os.path.join(work_dir, \"Clustering_Results\")\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "    def load_and_preprocess_data(self, train_file, val_file=None):\n",
    "        print(f\"Loading training data from: {train_file}\")\n",
    "        train_df = pd.read_excel(train_file)\n",
    "        \n",
    "        if 'Prognosis' in train_df.columns:\n",
    "            self.feature_cols = [c for c in train_df.columns if c != 'Prognosis']\n",
    "            self.label_col = 'Prognosis'\n",
    "        else:\n",
    "            self.feature_cols = train_df.columns.tolist()\n",
    "            self.label_col = None\n",
    "            \n",
    "        print(f\"Features ({len(self.feature_cols)}): {self.feature_cols}\")\n",
    "        \n",
    "        X_train = train_df[self.feature_cols].copy()\n",
    "        X_train = X_train.fillna(X_train.mean())\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        X_val_scaled = None\n",
    "        val_df = None\n",
    "        if val_file and os.path.exists(val_file):\n",
    "            print(f\"Loading validation data from: {val_file}\")\n",
    "            val_df = pd.read_excel(val_file)\n",
    "            X_val = val_df[self.feature_cols].copy()\n",
    "            X_val = X_val.fillna(X_train.mean())\n",
    "            X_val_scaled = self.scaler.transform(X_val)\n",
    "            \n",
    "        return X_train_scaled, X_val_scaled, train_df, val_df\n",
    "\n",
    "    def determine_optimal_clusters(self, X, max_k=8):\n",
    "        print(\"\\nDetermining optimal k...\")\n",
    "        k_range = range(2, max_k + 1)\n",
    "        scores = {'Silhouette': [], 'Calinski-Harabasz': [], 'Davies-Bouldin': []}\n",
    "        \n",
    "        for k in k_range:\n",
    "            gmm = GaussianMixture(n_components=k, random_state=self.random_state)\n",
    "            labels = gmm.fit_predict(X)\n",
    "            scores['Silhouette'].append(silhouette_score(X, labels))\n",
    "            scores['Calinski-Harabasz'].append(calinski_harabasz_score(X, labels))\n",
    "            scores['Davies-Bouldin'].append(davies_bouldin_score(X, labels))\n",
    "            \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        metrics = [('Silhouette', 'bo-'), ('Calinski-Harabasz', 'ro-'), ('Davies-Bouldin', 'go-')]\n",
    "        \n",
    "        for i, (name, style) in enumerate(metrics):\n",
    "            axes[i].plot(k_range, scores[name], style)\n",
    "            axes[i].set_title(name)\n",
    "            axes[i].set_xlabel('k')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"Optimal_K_Selection.pdf\"), format='pdf')\n",
    "        plt.close()\n",
    "        \n",
    "        optimal_k = k_range[np.argmax(scores['Silhouette'])]\n",
    "        print(f\"Optimal k: {optimal_k}\")\n",
    "        return optimal_k\n",
    "\n",
    "    def compare_clustering_methods(self, X):\n",
    "        print(\"\\nComparing algorithms...\")\n",
    "        results = []\n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                if name == 'GMM':\n",
    "                    model.fit(X)\n",
    "                    labels = model.predict(X)\n",
    "                else:\n",
    "                    labels = model.fit_predict(X)\n",
    "                \n",
    "                results.append({\n",
    "                    'Method': name,\n",
    "                    'Silhouette': silhouette_score(X, labels),\n",
    "                    'Calinski-Harabasz': calinski_harabasz_score(X, labels),\n",
    "                    'Davies-Bouldin': davies_bouldin_score(X, labels)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {name}: {e}\")\n",
    "                \n",
    "        res_df = pd.DataFrame(results)\n",
    "        print(res_df)\n",
    "        res_df.to_excel(os.path.join(self.output_dir, \"Method_Comparison.xlsx\"), index=False)\n",
    "        return res_df.sort_values('Silhouette', ascending=False).iloc[0]['Method']\n",
    "\n",
    "    def visualize_clusters_ica(self, X, labels, title, filename):\n",
    "        ica = FastICA(n_components=2, random_state=self.random_state)\n",
    "        X_ica = ica.fit_transform(X)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        scatter = plt.scatter(X_ica[:, 0], X_ica[:, 1], c=labels, cmap='viridis', alpha=0.7, s=60)\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.title(f'{title} (FastICA)')\n",
    "        plt.xlabel('ICA Component 1')\n",
    "        plt.ylabel('ICA Component 2')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, filename), format='pdf')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_feature_violins(self, df, labels, prefix):\n",
    "        \"\"\"绘制所有指标的小提琴图并进行 Mann-Whitney U 检验\"\"\"\n",
    "        data = df.copy()\n",
    "        data['Cluster'] = labels\n",
    "        \n",
    "        # 需要绘制的所有列：特征 + Prognosis\n",
    "        plot_cols = self.feature_cols + ([self.label_col] if self.label_col else [])\n",
    "        \n",
    "        # 确保只有2个簇才能做 Mann-Whitney U，否则只画图\n",
    "        unique_clusters = np.unique(labels)\n",
    "        is_binary = len(unique_clusters) == 2\n",
    "        \n",
    "        # 计算行数，每行4个图\n",
    "        n_cols = 4\n",
    "        n_rows = int(np.ceil(len(plot_cols) / n_cols))\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, col in enumerate(plot_cols):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # 绘制小提琴图\n",
    "            sns.violinplot(x='Cluster', y=col, data=data, ax=ax, palette=\"muted\", inner=\"box\")\n",
    "            ax.set_title(f'{col}')\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel('Value')\n",
    "            \n",
    "            # 如果是二分类聚类，进行统计检验\n",
    "            if is_binary:\n",
    "                group0 = data[data['Cluster'] == unique_clusters[0]][col]\n",
    "                group1 = data[data['Cluster'] == unique_clusters[1]][col]\n",
    "                \n",
    "                try:\n",
    "                    stat, p = mannwhitneyu(group0, group1)\n",
    "                    # 在图上标注 P 值\n",
    "                    y_max = data[col].max()\n",
    "                    ax.text(0.5, y_max, f'p={p:.3e}', ha='center', va='bottom', \n",
    "                            fontsize=10, color='red' if p < 0.05 else 'black')\n",
    "                except Exception as e:\n",
    "                    print(f\"Stat error on {col}: {e}\")\n",
    "                    \n",
    "        # 隐藏多余的子图\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, f\"{prefix}_Violin_Plots_with_P_Values.pdf\"), format='pdf')\n",
    "        plt.close()\n",
    "\n",
    "    def analyze_cluster_characteristics(self, df, labels, prefix=\"Train\"):\n",
    "        data = df.copy()\n",
    "        data['Cluster'] = labels\n",
    "        \n",
    "        # 保存数据\n",
    "        data.to_excel(os.path.join(self.output_dir, f\"{prefix}_Data_with_Clusters.xlsx\"), index=False)\n",
    "        \n",
    "        # 统计描述\n",
    "        summary = data.groupby('Cluster').agg(['mean', 'std'])\n",
    "        summary.to_excel(os.path.join(self.output_dir, f\"{prefix}_Feature_Stats.xlsx\"))\n",
    "        \n",
    "        # 绘制小提琴图 + 检验\n",
    "        self.plot_feature_violins(df, labels, prefix)\n",
    "\n",
    "        # 单独绘制 Prognosis 柱状图（更直观）\n",
    "        if self.label_col in data.columns:\n",
    "            mortality = data.groupby('Cluster')[self.label_col].mean()\n",
    "            plt.figure(figsize=(5, 4))\n",
    "            mortality.plot(kind='bar', color='coral', alpha=0.8, edgecolor='black')\n",
    "            plt.title(f'{prefix} - Prognosis Rate by Cluster')\n",
    "            plt.ylabel('Rate')\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_dir, f\"{prefix}_Prognosis_Bar.pdf\"), format='pdf')\n",
    "            plt.close()\n",
    "\n",
    "    def run(self, train_file, val_file):\n",
    "        # 1. Load\n",
    "        X_train_scaled, X_val_scaled, df_train, df_val = self.load_and_preprocess_data(train_file, val_file)\n",
    "        \n",
    "        # 2. Optimal K\n",
    "        self.n_clusters = self.determine_optimal_clusters(X_train_scaled)\n",
    "        for model in self.models.values():\n",
    "            if hasattr(model, 'n_components'): model.set_params(n_components=self.n_clusters)\n",
    "            elif hasattr(model, 'n_clusters'): model.set_params(n_clusters=self.n_clusters)\n",
    "            \n",
    "        # 3. Best Method\n",
    "        best_method = self.compare_clustering_methods(X_train_scaled)\n",
    "        best_model = self.models[best_method]\n",
    "        \n",
    "        # 4. Train Cluster\n",
    "        print(f\"\\nClustering Train set with {best_method}...\")\n",
    "        if best_method == 'GMM':\n",
    "            best_model.fit(X_train_scaled)\n",
    "            train_labels = best_model.predict(X_train_scaled)\n",
    "        else:\n",
    "            train_labels = best_model.fit_predict(X_train_scaled)\n",
    "            \n",
    "        self.visualize_clusters_ica(X_train_scaled, train_labels, \"Train Clusters\", \"Train_ICA.pdf\")\n",
    "        self.analyze_cluster_characteristics(df_train, train_labels, \"Train\")\n",
    "        \n",
    "        # 5. Validation Cluster\n",
    "        if X_val_scaled is not None:\n",
    "            print(f\"\\nClustering Validation set...\")\n",
    "            if hasattr(best_model, 'predict'):\n",
    "                val_labels = best_model.predict(X_val_scaled)\n",
    "            else:\n",
    "                val_labels = best_model.fit_predict(X_val_scaled)\n",
    "            \n",
    "            self.visualize_clusters_ica(X_val_scaled, val_labels, \"Validation Clusters\", \"Val_ICA.pdf\")\n",
    "            self.analyze_cluster_characteristics(df_val, val_labels, \"Val\")\n",
    "            \n",
    "        print(f\"\\nDone. Results in: {self.output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    WORK_DIR = r\"D:\\卒中预后第一篇论文\\2025.11.25-new work\\6.无监督聚类\"\n",
    "    TRAIN_FILE = os.path.join(WORK_DIR, \"训练集-量子计算.xlsx\")\n",
    "    VAL_FILE = os.path.join(WORK_DIR, \"外部验证集-量子计算.xlsx\")\n",
    "    \n",
    "    clustering = StrokeSubphenotypeClustering(WORK_DIR)\n",
    "    clustering.run(TRAIN_FILE, VAL_FILE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
